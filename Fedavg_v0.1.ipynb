{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used as a baseline for comparing diffetent types of privacy-preservation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/grey/ENV3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.7.1)\r\n",
      "Requirement already satisfied: numpy in /Users/grey/ENV3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.24.2)\r\n",
      "Requirement already satisfied: pandas in /Users/grey/ENV3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.5.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/grey/ENV3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.2.2)\r\n",
      "Requirement already satisfied: torch in /Users/grey/ENV3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.0.1)\r\n",
      "Requirement already satisfied: cloudmesh-common in /Users/grey/Desktop/github/cloudmesh-community/cm/cloudmesh-common (from -r requirements.txt (line 6)) (4.3.173)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.0.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (4.38.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (23.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (9.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/grey/ENV3/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2022.5)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/grey/ENV3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.9.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/grey/ENV3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.1.0)\r\n",
      "Requirement already satisfied: filelock in /Users/grey/ENV3/lib/python3.11/site-packages (from torch->-r requirements.txt (line 5)) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/grey/ENV3/lib/python3.11/site-packages (from torch->-r requirements.txt (line 5)) (4.4.0)\r\n",
      "Requirement already satisfied: sympy in /Users/grey/ENV3/lib/python3.11/site-packages (from torch->-r requirements.txt (line 5)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/grey/ENV3/lib/python3.11/site-packages (from torch->-r requirements.txt (line 5)) (2.8.7)\r\n",
      "Requirement already satisfied: jinja2 in /Users/grey/ENV3/lib/python3.11/site-packages (from torch->-r requirements.txt (line 5)) (3.1.2)\r\n",
      "Requirement already satisfied: psutil in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (5.9.3)\r\n",
      "Requirement already satisfied: python-hostlist in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (1.22)\r\n",
      "Requirement already satisfied: simplejson in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (3.17.6)\r\n",
      "Requirement already satisfied: oyaml in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (1.0)\r\n",
      "Requirement already satisfied: colorama in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (0.4.4)\r\n",
      "Requirement already satisfied: humanize in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (4.4.0)\r\n",
      "Requirement already satisfied: tabulate in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (0.9.0)\r\n",
      "Requirement already satisfied: requests in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (2.28.2)\r\n",
      "Requirement already satisfied: pyfiglet in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (0.8.post1)\r\n",
      "Requirement already satisfied: tqdm in /Users/grey/ENV3/lib/python3.11/site-packages (from cloudmesh-common->-r requirements.txt (line 6)) (4.64.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/grey/ENV3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/grey/ENV3/lib/python3.11/site-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.1)\r\n",
      "Requirement already satisfied: pyyaml in /Users/grey/ENV3/lib/python3.11/site-packages (from oyaml->cloudmesh-common->-r requirements.txt (line 6)) (5.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/grey/ENV3/lib/python3.11/site-packages (from requests->cloudmesh-common->-r requirements.txt (line 6)) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/grey/ENV3/lib/python3.11/site-packages (from requests->cloudmesh-common->-r requirements.txt (line 6)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/grey/ENV3/lib/python3.11/site-packages (from requests->cloudmesh-common->-r requirements.txt (line 6)) (1.26.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/grey/ENV3/lib/python3.11/site-packages (from requests->cloudmesh-common->-r requirements.txt (line 6)) (2022.9.24)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/grey/ENV3/lib/python3.11/site-packages (from sympy->torch->-r requirements.txt (line 5)) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improting the required libraries.\n",
    "user = \"gregor\"\n",
    "node = \"mac-m1-max\"\n",
    "from cloudmesh.common.StopWatch import StopWatch\n",
    "from cloudmesh.common.FlatDict import FlatDict\n",
    "from cloudmesh.common.util import banner\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "\n",
    "# ours\n",
    "from approach.lstm_forecast import ShallowForecastLSTM\n",
    "from util.utils import train_one_step, test_model, predict, SequenceDataset\n",
    "from util.fed_utils import FedAvg_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Context: time series data, decentralized on \"nodes\".\n",
    "- Constraints: the nodes cannot exchange their data.\n",
    "- Goal: train a predictive model (here forecasting) on each node.\n",
    "\n",
    "Federated learning: take advantage of the parameters of the models of the other nodes to improve the predictive capacities of each model (local). # FedAvg reference + two-sentence description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Data: time series (one per node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series is a sequence of measurements indexed by time. When at each of these instants of time, a single state variable is measured (e.g. temperature, frequentation of a bus station, etc.), the time series is said to be univariate. A fortiori, when several variables are measured at a given moment, we say that the time series is multivariate (or multivalued).\n",
    "\n",
    "We note $x_k(t)$ the sample measured on the variable $k$ at time $t$ for $k = 1 \\dots K$ and $t = 0, \\dots T$.\n",
    "\n",
    "Often, when we study a multivariate time series, we conjecture that the variables $K$ (or a subset) are correlated with each other over time, that is to say that the variables $x_{k_1}$ and $x_{k_2}$ evolve together. In this case, it assumes that to predict the future value $x_{k_1}(T+1)$ (unmeasured) from its past values $x_{k_1}(t=0) \\dots x_{k_1}( t=T)$, we must also consider the passed values $x_{k_2}(t=0) \\dots x_{k_2}(t=T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The prediction model (forecasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to train a model to predict the future values of each of the variables (forecasting), i.e. give an estimate of $x_{1}(T+1), \\dots x_{k}(T +1) \\dots x_{K}(T+1)$. Each model $k$ (a Long Short Term Memory type neural network) will then attempt to approximate a function $f_k$ such that $x_{t+1} = f_k (x_t, \\dots x_{t-1} \\ dots )$. The prediction function $f_k$ represents the model $k$ and has, say $p$ parameters $w_{1,k}, \\dots w_{1,p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Federated learning context\n",
    "\n",
    "In our context, we assume that the data of a variable $k$ are measured on a node $k$. This data cannot be communicated and shared with other nodes. It is therefore impossible to build a global model from the data of all the variables. We will therefore train one model per node. In order to take these possible correlations into consideration, each model can be trained by sharing not the data but the parameters of each model with a central server. A simple strategy, called Federated Averaging, consists of averaging the parameters of each of the models, more concretely:\n",
    "- (1.) the local model $k$ learns on its data $x_{k}(t=0) \\dots x_{k}(t=T)$,\n",
    "- (2.) the model $k$ sends its parameters, say $w_{1,k}, \\dots w_{p,k}$ to the central server,\n",
    "- (3.) the central server calculates the average $w_{1}^c, \\dots w_{p}^c$ of the $p$ parameters of the $K$ models: e.g. for a parameter $w_{p}^c = \\frac{1}{K} \\sum_{i=1}^{K} w_{p,i}$,\n",
    "- (4.) the central server returns the averaged parameters to node $k$,\n",
    "- (5.) the local model $k$ receives the parameters $w_{1}^c, \\dots w_{p}^c$, and starts again at step (1.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliographic reference of the Federated Averaging algorithm: H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Agüera y Arcas, “Communication-Efﬁcient Learning of Deep Networks from Decentralized Data,” in AISTATS, 2017, vol. 54."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of the notebook:\n",
    "- apply Federated Averaging to predict over time (forecaster) on each node,\n",
    "- compare to non-federated learning (the nodes do not communicate with each other),\n",
    "- compare to \"classical\" centralized learning, where the data for each of the variables is not distributed and is used to train a single model on a \"big\" multivariate time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is assumed that each node contains data from a single time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type load\n"
     ]
    }
   ],
   "source": [
    "StopWatch.start(\"total\")\n",
    "StopWatch.start(\"load-data\")\n",
    "config = FlatDict()\n",
    "config.load(\"config.yaml\")\n",
    "\n",
    "x_data = np.load(\"./datasets/electricity.npy\")\n",
    "T_size, p = x_data.shape # output is (10560, 963)\n",
    "n_clients = 10 # 50 # reduced number for the example\n",
    "t_grid = np.arange(0, T_size) # output is array([    0,     1,     2, ..., 10557, 10558, 10559])\n",
    "\n",
    "# common to all experiments\n",
    "T_train = int(0.70 * T_size) # output is 7391\n",
    "lag = 40 # number of time-consecutive samples within a mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# config.yaml\n",
      "# ----------------------------------------------------------------------\n",
      "\n",
      "{'experiment.epoch': 20, 'experiment.batch_size': 8, 'sep': '.'}\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# X DATA\n",
      "# ----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "             0           1           2           3           4            5    \\\n0      16.497462   91.038407  309.296264  552.845528  289.024390  1392.857143   \n1      19.035533   92.460882  309.296264  548.780488  280.487805  1357.142857   \n2      16.497462   91.749644  309.296264  546.747967  265.853659  1145.833333   \n3      19.035533   88.193457  309.296264  408.536585  201.219512   767.857143   \n4      17.766497   87.482219  309.296264  327.235772  180.487805   622.023810   \n...          ...         ...         ...         ...         ...          ...   \n26299   7.614213  110.241821    6.950478  821.138211  385.365854  1532.738095   \n26300   8.883249   97.439545    6.950478  713.414634  359.756098  1324.404762   \n26301  10.152284   90.327169    6.950478  650.406504  351.219512  1267.857143   \n26302   8.883249   88.193457    6.950478  630.081301  339.024390  1247.023810   \n26303   8.883249   83.214794    6.950478  674.796748  334.146341  1208.333333   \n\n             6            7           8           9    ...          360  \\\n0      34.482759  1117.845118  291.958042  351.612903  ...   482.512491   \n1      27.699265  1104.377104  269.230769  329.032258  ...   451.106353   \n2      26.568683   959.595960  255.244755  288.172043  ...   410.421128   \n3      19.219898   801.346801  166.083916  184.946237  ...   259.814418   \n4      14.132278   720.538721  183.566434  169.892473  ...   295.503212   \n...          ...          ...         ...         ...  ...          ...   \n26299  47.484454  1380.471380  412.587413  343.010753  ...  1501.784440   \n26300  45.223290  1188.552189  351.398601  313.978495  ...  1317.630264   \n26301  46.353872  1074.074074  293.706294  276.344086  ...  1309.064954   \n26302  45.223290  1026.936027  283.216783  292.473118  ...  1178.443969   \n26303  41.831543   969.696970  253.496503  288.172043  ...   999.286224   \n\n            361          362          363         364        365          366  \\\n0      103900.0  6042.194093  6636.363636   62.581486  51.492101  2096.575944   \n1       95500.0  4388.185654  6613.636364   63.885267  42.129901  2200.175593   \n2       96600.0  4278.481013  6568.181818   63.885267  38.619075  1996.488147   \n3       93400.0  4168.776371  6318.181818   73.011734  38.619075  1320.456541   \n4       90300.0  4130.801688  6272.727273   62.581486  35.108250  1314.310799   \n...         ...          ...          ...         ...        ...          ...   \n26299  163700.0  8776.371308  9568.181818  284.224250  19.894675  1840.210711   \n26300  157900.0  6586.497890  8340.909091  298.565841  16.383850  1357.330992   \n26301  154200.0  6590.717300  5704.545455  294.654498  17.554125  2047.410009   \n26302  133100.0  6514.767932  5454.545455  220.338983  30.427150  2796.312555   \n26303  111900.0  5978.902954  4522.727273  110.821382  27.501463  2635.645303   \n\n              367          368           369  \n0      243.739566  2854.105572      0.000000  \n1      245.409015  2823.313783      0.000000  \n2      203.672788  2756.598240      0.000000  \n3      188.647746  2724.340176      0.000000  \n4      190.317195  2741.202346      0.000000  \n...           ...          ...           ...  \n26299  160.267112  2764.662757  34918.918919  \n26300  525.876461  2722.873900  32864.864865  \n26301  661.101836  2647.360704  33189.189189  \n26302  784.641068  2709.677419  30108.108108  \n26303  601.001669  2659.090909  27243.243243  \n\n[26304 rows x 370 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>360</th>\n      <th>361</th>\n      <th>362</th>\n      <th>363</th>\n      <th>364</th>\n      <th>365</th>\n      <th>366</th>\n      <th>367</th>\n      <th>368</th>\n      <th>369</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.497462</td>\n      <td>91.038407</td>\n      <td>309.296264</td>\n      <td>552.845528</td>\n      <td>289.024390</td>\n      <td>1392.857143</td>\n      <td>34.482759</td>\n      <td>1117.845118</td>\n      <td>291.958042</td>\n      <td>351.612903</td>\n      <td>...</td>\n      <td>482.512491</td>\n      <td>103900.0</td>\n      <td>6042.194093</td>\n      <td>6636.363636</td>\n      <td>62.581486</td>\n      <td>51.492101</td>\n      <td>2096.575944</td>\n      <td>243.739566</td>\n      <td>2854.105572</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.035533</td>\n      <td>92.460882</td>\n      <td>309.296264</td>\n      <td>548.780488</td>\n      <td>280.487805</td>\n      <td>1357.142857</td>\n      <td>27.699265</td>\n      <td>1104.377104</td>\n      <td>269.230769</td>\n      <td>329.032258</td>\n      <td>...</td>\n      <td>451.106353</td>\n      <td>95500.0</td>\n      <td>4388.185654</td>\n      <td>6613.636364</td>\n      <td>63.885267</td>\n      <td>42.129901</td>\n      <td>2200.175593</td>\n      <td>245.409015</td>\n      <td>2823.313783</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.497462</td>\n      <td>91.749644</td>\n      <td>309.296264</td>\n      <td>546.747967</td>\n      <td>265.853659</td>\n      <td>1145.833333</td>\n      <td>26.568683</td>\n      <td>959.595960</td>\n      <td>255.244755</td>\n      <td>288.172043</td>\n      <td>...</td>\n      <td>410.421128</td>\n      <td>96600.0</td>\n      <td>4278.481013</td>\n      <td>6568.181818</td>\n      <td>63.885267</td>\n      <td>38.619075</td>\n      <td>1996.488147</td>\n      <td>203.672788</td>\n      <td>2756.598240</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19.035533</td>\n      <td>88.193457</td>\n      <td>309.296264</td>\n      <td>408.536585</td>\n      <td>201.219512</td>\n      <td>767.857143</td>\n      <td>19.219898</td>\n      <td>801.346801</td>\n      <td>166.083916</td>\n      <td>184.946237</td>\n      <td>...</td>\n      <td>259.814418</td>\n      <td>93400.0</td>\n      <td>4168.776371</td>\n      <td>6318.181818</td>\n      <td>73.011734</td>\n      <td>38.619075</td>\n      <td>1320.456541</td>\n      <td>188.647746</td>\n      <td>2724.340176</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.766497</td>\n      <td>87.482219</td>\n      <td>309.296264</td>\n      <td>327.235772</td>\n      <td>180.487805</td>\n      <td>622.023810</td>\n      <td>14.132278</td>\n      <td>720.538721</td>\n      <td>183.566434</td>\n      <td>169.892473</td>\n      <td>...</td>\n      <td>295.503212</td>\n      <td>90300.0</td>\n      <td>4130.801688</td>\n      <td>6272.727273</td>\n      <td>62.581486</td>\n      <td>35.108250</td>\n      <td>1314.310799</td>\n      <td>190.317195</td>\n      <td>2741.202346</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26299</th>\n      <td>7.614213</td>\n      <td>110.241821</td>\n      <td>6.950478</td>\n      <td>821.138211</td>\n      <td>385.365854</td>\n      <td>1532.738095</td>\n      <td>47.484454</td>\n      <td>1380.471380</td>\n      <td>412.587413</td>\n      <td>343.010753</td>\n      <td>...</td>\n      <td>1501.784440</td>\n      <td>163700.0</td>\n      <td>8776.371308</td>\n      <td>9568.181818</td>\n      <td>284.224250</td>\n      <td>19.894675</td>\n      <td>1840.210711</td>\n      <td>160.267112</td>\n      <td>2764.662757</td>\n      <td>34918.918919</td>\n    </tr>\n    <tr>\n      <th>26300</th>\n      <td>8.883249</td>\n      <td>97.439545</td>\n      <td>6.950478</td>\n      <td>713.414634</td>\n      <td>359.756098</td>\n      <td>1324.404762</td>\n      <td>45.223290</td>\n      <td>1188.552189</td>\n      <td>351.398601</td>\n      <td>313.978495</td>\n      <td>...</td>\n      <td>1317.630264</td>\n      <td>157900.0</td>\n      <td>6586.497890</td>\n      <td>8340.909091</td>\n      <td>298.565841</td>\n      <td>16.383850</td>\n      <td>1357.330992</td>\n      <td>525.876461</td>\n      <td>2722.873900</td>\n      <td>32864.864865</td>\n    </tr>\n    <tr>\n      <th>26301</th>\n      <td>10.152284</td>\n      <td>90.327169</td>\n      <td>6.950478</td>\n      <td>650.406504</td>\n      <td>351.219512</td>\n      <td>1267.857143</td>\n      <td>46.353872</td>\n      <td>1074.074074</td>\n      <td>293.706294</td>\n      <td>276.344086</td>\n      <td>...</td>\n      <td>1309.064954</td>\n      <td>154200.0</td>\n      <td>6590.717300</td>\n      <td>5704.545455</td>\n      <td>294.654498</td>\n      <td>17.554125</td>\n      <td>2047.410009</td>\n      <td>661.101836</td>\n      <td>2647.360704</td>\n      <td>33189.189189</td>\n    </tr>\n    <tr>\n      <th>26302</th>\n      <td>8.883249</td>\n      <td>88.193457</td>\n      <td>6.950478</td>\n      <td>630.081301</td>\n      <td>339.024390</td>\n      <td>1247.023810</td>\n      <td>45.223290</td>\n      <td>1026.936027</td>\n      <td>283.216783</td>\n      <td>292.473118</td>\n      <td>...</td>\n      <td>1178.443969</td>\n      <td>133100.0</td>\n      <td>6514.767932</td>\n      <td>5454.545455</td>\n      <td>220.338983</td>\n      <td>30.427150</td>\n      <td>2796.312555</td>\n      <td>784.641068</td>\n      <td>2709.677419</td>\n      <td>30108.108108</td>\n    </tr>\n    <tr>\n      <th>26303</th>\n      <td>8.883249</td>\n      <td>83.214794</td>\n      <td>6.950478</td>\n      <td>674.796748</td>\n      <td>334.146341</td>\n      <td>1208.333333</td>\n      <td>41.831543</td>\n      <td>969.696970</td>\n      <td>253.496503</td>\n      <td>288.172043</td>\n      <td>...</td>\n      <td>999.286224</td>\n      <td>111900.0</td>\n      <td>5978.902954</td>\n      <td>4522.727273</td>\n      <td>110.821382</td>\n      <td>27.501463</td>\n      <td>2635.645303</td>\n      <td>601.001669</td>\n      <td>2659.090909</td>\n      <td>27243.243243</td>\n    </tr>\n  </tbody>\n</table>\n<p>26304 rows × 370 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(x_data)\n",
    "StopWatch.stop(\"load-data\")\n",
    "\n",
    "banner(\"config.yaml\")\n",
    "print(config)\n",
    "\n",
    "banner(\"X DATA\")\n",
    "\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20 # 100\n",
    "batch_size = 2**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs of the data of some nodes $x_0 \\dots x_{15}$ as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 16 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 4, sharex=True, sharey=True)\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].set_ylim((-0.1, 1.1))\n",
    "for k in range(16): # 16 clients (nodes)\n",
    "    axs[k].plot(\n",
    "        t_grid, x_data[:, k],\n",
    "        alpha=0.4\n",
    "        )\n",
    "    axs[k].set_ylabel(f'x{k}', labelpad=-0.5)\n",
    "    axs[k].set_xlabel('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([309.29626412, 309.29626412, 309.29626412, ...,   6.95047785,\n         6.95047785,   6.95047785])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(18412,)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[:T_train, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(7892,)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[T_train:, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. NON-federated workouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: for each indexed variable $k$, $x_1 \\dots x_{K}$, we want to train a model that predicts $x_k(t+1)$ according to its past values $x_k(t), x_k(t-1 ), \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train one model per node, independently of the (data and models) of the other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating one dataset per variable (which is naturally the case in a federated context). Each dataset is then divided into two sets training/testing then centered-reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWatch.start(\"non-federated-workouts\")\n",
    "train_loaders, test_loaders = [], []\n",
    "\n",
    "# one scaler for each node\n",
    "scalers =  [StandardScaler(with_mean=True, with_std=True) for k in range(n_clients)]\n",
    "\n",
    "for k in range(n_clients):\n",
    "    \n",
    "    train_loaders.append(\n",
    "        DataLoader(\n",
    "            SequenceDataset(\n",
    "                scalers[k].fit_transform(x_data[:T_train, k][:, np.newaxis]), # center-reduce train data of client k\n",
    "                lag=lag\n",
    "            ),\n",
    "            batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_loaders.append(\n",
    "        DataLoader(\n",
    "            SequenceDataset(\n",
    "                scalers[k].transform(x_data[T_train:, k][:, np.newaxis]),\n",
    "                lag=lag\n",
    "            ),\n",
    "            batch_size=batch_size, shuffle=False # makes the data same when batches are requested at different times\n",
    "        )\n",
    "    )\n",
    "StopWatch.stop(\"non-federated-workouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a one-layer Long Short Time Memory (LSTM) recurrent neural network model with a small number of neurons (for the example). Each node will have the same model architecture but will be trained on its own data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desgin the model\n",
    "StopWatch.start(\"design-model\")\n",
    "num_hidden_units = 5 # 50\n",
    "models_nofed = [ShallowForecastLSTM(num_var=1, hidden_units=num_hidden_units) for k in range(n_clients)]\n",
    "StopWatch.stop(\"design-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fix the training procedure which itself involves the choice of an error function to minimize, a minimizer, a learning step, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Learning model of node#0==\n",
      "------\n",
      " Epoch 0\n",
      "Train loss: 0.24630350002616527\n",
      "Test loss: 0.1337969005614469\n",
      "------\n",
      " Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "StopWatch.start(\"train-model\")\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizers = [Adam(\n",
    "    models_nofed[k].parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.60, 0.75),\n",
    "    weight_decay=1e-2\n",
    ") for k in range(n_clients)]\n",
    "\n",
    "# self-tuning of the learning step at each iteration on each client\n",
    "schedulers = [ReduceLROnPlateau(optimizers[k], 'min', patience = 7) for k in range(n_clients)]\n",
    "\n",
    "# \n",
    "train_loss_epoch = np.empty((n_epoch, n_clients))\n",
    "test_loss_epoch = np.empty_like(train_loss_epoch)\n",
    "\n",
    "# for each client\n",
    "for k in range(n_clients):\n",
    "    print(f'==Learning model of node#{k}==')\n",
    "\n",
    "    # for each learning step\n",
    "    for i_epoch in range(n_epoch):\n",
    "        print(f'------\\n Epoch {i_epoch}')\n",
    "\n",
    "        # train\n",
    "        train_loss_epoch[i_epoch, k], model_k = train_one_step(\n",
    "            train_loaders[k], models_nofed[k], loss_function, optimizer=optimizers[k]\n",
    "            )\n",
    "\n",
    "        # test\n",
    "        test_loss_epoch[i_epoch, k] = test_model(\n",
    "            test_loaders[k], models_nofed[k], loss_function, verbose=True)\n",
    "\n",
    "        # learning rate decay\n",
    "        schedulers[k].step(test_loss_epoch[i_epoch, k])\n",
    "        \n",
    "        if i_epoch == n_epoch - 1:\n",
    "            models_nofed[k] = deepcopy(models_nofed[k])\n",
    "StopWatch.stop(\"train-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# average testing loss (across clients) along the iterations\n",
    "print('Global testing loss')\n",
    "StopWatch.start(\"train-loss\")\n",
    "np.average(test_loss_epoch, axis=1, weights=np.ones(n_clients))\n",
    "StopWatch.stop(\"train-loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of training/test errors per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for k in range(n_clients):\n",
    "    # make a function\n",
    "    plt.plot(train_loss_epoch[:, k], label=f'Train, node#{k}', color=f'C{k}', alpha=0.5, linestyle='dashed')\n",
    "    plt.plot(test_loss_epoch[:, k], label=f'Test, node#{k}', color=f'C{k}', alpha=0.5, linestyle='solid')\n",
    "\n",
    "plt.title('Train/test losses wrt iteration number')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of predictions/true values on training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "t_train, t_test = np.arange(0, T_train), np.arange(T_train, T_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_client_pred_truth(idx_client, models, train_samples=True, timesteps=t_train):\n",
    "    \"\"\"plot samples vs predictions\"\"\"\n",
    "\n",
    "    # compute prediction\n",
    "    if train_samples:\n",
    "        x_pred = scalers[k].inverse_transform(predict(train_loaders[k], models[k]).numpy())\n",
    "    else:\n",
    "        x_pred = scalers[k].inverse_transform(predict(test_loaders[k], models[k]).numpy())\n",
    "\n",
    "    # plot the time series\n",
    "    if train_samples:\n",
    "        plt.plot(timesteps, x_data[:T_train, idx_client], marker='o', label='Truth', color='C0', alpha=0.25)\n",
    "    else:\n",
    "        plt.plot(timesteps, x_data[T_train:, idx_client], label='Truth',  marker='o', color='C0', alpha=0.25)\n",
    "\n",
    "    plt.plot(timesteps, x_pred, label='Prediction', color='red', alpha=0.25)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of the time series (test data) of the customer $k$ and the associated predictions of the model $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_client_pred_truth(\n",
    "    idx_client=4,\n",
    "    models=models_nofed,\n",
    "    train_samples=False,\n",
    "    timesteps=t_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Federated training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: for each indexed variable $k$, $x_1 \\dots x_{K}$, we want to train a model that predicts $x_k(t+1)$ according to its past values $x_k(t), x_k(t-1 ), \\dots$ in the federated context. The local models (nodes) are first trained independently of each other (as in I.), then they share their model (weight of the LSTM) with the central server (federator) which itself aggregates the models (weight $\\ textit of $LSTM). This aggregated model is returned to each client that iterates training locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iterative process, called “communication round”, between the local nodes and the central server, can be repeated several times (n_cr times).\n",
    "n_cr: number of communication rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "StopWatch.start(\"federated-learning\")\n",
    "models_fed, local_losses_bfr_fedavg, local_losses_aft_fedavg, global_loss  = FedAvg_loop(\n",
    "    call_basemodel=ShallowForecastLSTM,\n",
    "    archi_basemodel={'n_inputs': 1, 'n_hidden': 10},\n",
    "    train_sets=train_loaders,\n",
    "    test_sets=test_loaders,\n",
    "    n_cr=5, n_local_epochs=n_epoch,\n",
    "    lr= 10**-2\n",
    ")\n",
    "StopWatch.stop(\"federated-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_client_pred_truth(\n",
    "    idx_client=1,\n",
    "    models=models_fed,\n",
    "    train_samples=False,\n",
    "    timesteps=t_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Central drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we leave the decentralized context, where we train one model per node. This time, we are in the classic case: the data form a complete centralized dataset. A model is trained from this complete dataset (all data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, correlations between variables $k=1 \\dots K$ influence the learned model. It is important to understand that in stages I. and II. these correlations are ignored by construction (consequently the models of stages I. and II. are based only on the observed values of each of the variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# scale the data\n",
    "## one scaler for the whole dataset as we only have a centralized one\n",
    "scaler =  StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    scaler.fit_transform(x_data[:T_train, :n_clients]), lag=lag\n",
    "    )\n",
    "test_dataset = SequenceDataset(\n",
    "    scaler.transform(x_data[T_train:, :n_clients]), lag=lag\n",
    "    )\n",
    "\n",
    "batch_size = 2**3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_hidden_units = 5 #50\n",
    "model_cent = ShallowForecastLSTM(num_var=n_clients, hidden_units=num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "StopWatch.start(\"train-model-central-drive\")\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = Adam(model_cent.parameters(), lr=learning_rate, betas=(0.6, 0.75))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 5)\n",
    "\n",
    "print(\"Untrained test\\n--------\")\n",
    "test_model(test_loader, model_cent, loss_function)\n",
    "\n",
    "train_loss_epoch = np.empty(n_epoch)\n",
    "test_loss_epoch = np.empty(n_epoch)\n",
    "\n",
    "for i_epoch in range(n_epoch):\n",
    "    print(f\"Epoch {i_epoch}\\n---------\")\n",
    "\n",
    "    train_loss_epoch[i_epoch] = train_one_step(\n",
    "        train_loader, model_cent, loss_function, optimizer=optimizer\n",
    "        )[0]\n",
    "    test_loss_epoch[i_epoch] = test_model(test_loader, model_cent, loss_function)\n",
    "    scheduler.step(test_loss_epoch[i_epoch])\n",
    "StopWatch.stop(\"train-model-central-drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "StopWatch.stop(\"total\")\n",
    "StopWatch.benchmark(user=user, node=node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5ed5d6fbc1a2f3e5750ad773d65c4e93db1542b6fbaf6e3c60ee4b6784f8ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
